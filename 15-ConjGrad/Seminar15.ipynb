{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Семинар 15\n",
    "# Метод сопряжённых градиентов (Conjugate gradient method): гадкий утёнок "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## На прошлом семинаре\n",
    "\n",
    "1. Метод Ньютона\n",
    "2. Теоремы сходимости\n",
    "4. Сравнение с градиентным спуском\n",
    "5. Квазиньютоновские методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Система линейных уравнений vs. задача безусловной миимизации\n",
    "Рассмотрим задачу\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}x^{\\top}Ax - b^{\\top}x,\n",
    "$$\n",
    "где $A \\in \\mathbb{S}^n_{++}$.\n",
    "Из необходимого условия экстремума имеем\n",
    "$$\n",
    "Ax^* = b\n",
    "$$\n",
    "Также обозначим $f'(x_k) = Ax_k - b = r_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Как решать систему линейных уравнений $Ax = b$?\n",
    "\n",
    "- Прямые методы основаны на матричных разложениях:\n",
    "    - Плотная матрица $A$: для размерностей не больше нескольких тысяч\n",
    "    - Разреженная (sparse) матрица $A$: для размерностей порядка $10^4 - 10^5$\n",
    "- Итерационные методы: хороши для многих случаев, единственный подход для задач с размерностью $10^6$ и выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Немного истории...\n",
    "\n",
    "M. Hestenes и E. Stiefel предложили *метод сопряжённых градиентов* для решения систем линейных уравнений в 1952 году как **прямой** метод. \n",
    "\n",
    "Также долгое время считалось, что метод представляет только теоретический интерес поскольку\n",
    "- метод сопряжённых градиентов не работает на логарифмической линейке\n",
    "- метод сопряжённых градиентов имеет небольшое преимущество перед исключением Гаусса при вычислениях на калькуляторе \n",
    "\n",
    "Метод сопряжённых градиентов необходимо рассматривать как итерационный метод!\n",
    "\n",
    "Подробнее [здесь](https://www.siam.org/meetings/la09/talks/oleary.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Метод сопряжённых направлений\n",
    "\n",
    "В градиентном спуске предлагалось двигаться вдоль градиентов, но это замедляло сходимость для функций с вытянутыми линиями уровня.\n",
    "\n",
    "**Идея:** двигаться вдоль направлений, которые гарантируют сходимость за $n$ шагов.\n",
    "\n",
    "**Определение.** Множество ненулевых векторов $\\{p_0, \\ldots, p_l\\}$ называется *сопряжённым* относительно матрицы $A \\in \\mathbb{S}^n_{++}$, если \n",
    "$$\n",
    "p^{\\top}_iAp_j = 0, \\qquad i \\neq j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Утверждение.** Для любой $x_0 \\in \\mathbb{R}^n$ последовательность $\\{x_k\\}$, генерируемая методом сопряжённых направлений, сходится к решению системы $Ax = b$ максимум за $n$ шагов.\n",
    "\n",
    "```python\n",
    "def ConjugateDirections(x0, A, b, p):\n",
    "    x = x0\n",
    "    r = A.dot(x) - b\n",
    "    for i in xrange(len(p)):\n",
    "        alpha = - (r.dot(p[i])) / (p[i].dot(A.dot(p[i])))\n",
    "        x = x + alpha * p[i]\n",
    "        r = A.dot(x) - b\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Примеры сопряжённых направлений\n",
    "\n",
    "- Собственные векторы матрицы $A$\n",
    "- Для любого набора из $n$ векторов можно провести аналог ортогонализации Грамма-Шмидта и получить сопряжённые направления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Геометрическая интерпретация (Mathematics Stack Exchange)\n",
    "\n",
    "<center><img src=\"./cg.png\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Метод сопряжённых градиентов\n",
    "\n",
    "**Идея:** новое направление $p_k$ ищется в виде $p_k = -r_k + \\beta_k p_{k-1}$, где $\\beta_k$ выбирается, исходя из требования сопряжённости $p_k$ и $p_{k-1}$:\n",
    "$$\n",
    "\\beta_k = \\dfrac{p^{\\top}_{k-1}Ar_k}{p^{\\top}_{k-1}Ap^{\\top}_{k-1}}\n",
    "$$\n",
    "Таким образом, для получения следующего сопряжённого направления необходимо хранить только сопряжённое направление и остаток с предыдущей итерации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Теоремы сходимости\n",
    "\n",
    "**Теорема 1.** Если матрица $A$ имеет только $r$ различных собственных значений, то метод сопряжённых градиентов мойдётся за $r$ итераций.\n",
    "\n",
    "\n",
    "**Теорема 2.** Имеет место следующая оценка сходимости\n",
    "$$\n",
    "\\| x_{k+1} - x^* \\|_A \\leq \\left( \\dfrac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\|x_0 - x^*\\|_A,\n",
    "$$\n",
    "где $\\|x\\|_A = x^{\\top}Ax$ и $\\kappa(A) = \\frac{\\lambda_n(A)}{\\lambda_1(A)}$ - число обусловленности матрицы $A$\n",
    "\n",
    "**Замечание:** сравните коэффициент геометрической прогрессии с аналогом в градиентном спуске."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Интерпретации метода сопряжённых градиентов\n",
    "\n",
    "- Градиентный спуск в пространстве $y = Sx$, где $S = [p_0, \\ldots, p_n]$, в котором матрица $A$ становится диагональной (или единичной в случае ортонормированности сопряжённых направлений)\n",
    "- Поиск оптимального решения в [Крыловском подпространстве](https://stanford.edu/class/ee364b/lectures/conj_grad_slides.pdf) $\\mathcal{K}(A) = \\{b, Ab, A^2b, \\ldots \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Улучшенная версия метода сопряжёных градиентов\n",
    "На практике используются следующие формулы для шага $\\alpha_k$ и коэффициента $\\beta_{k}$:\n",
    "$$\n",
    "\\alpha_k = \\dfrac{r^{\\top}_k r_k}{p^{\\top}_{k}Ap^{\\top}_{k}} \\qquad \\beta_k = \\dfrac{r^{\\top}_k r_k}{r^{\\top}_{k-1} r_{k-1}}\n",
    "$$\n",
    "**Вопрос:** чем они лучше базовой версии?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Псевдокод метода сопряжённых градиентов\n",
    "```python\n",
    "def ConjugateGradientQuadratic(x0, A, b):\n",
    "    r = A.dot(x0) - b\n",
    "    p = -r\n",
    "    while np.linalg.norm(r) != 0:\n",
    "        alpha = r.dot(r) / p.dot(A.dot(p))\n",
    "        x = x + alpha * p\n",
    "        r_next = r + alpha * A.dot(p)\n",
    "        beta = r_next.dot(r_next) / r.dot(r)\n",
    "        p = -r_next + beta * p\n",
    "        r = r_next\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Метод сопряжённых градиентов для неквадратичной функции\n",
    "**Идея:** использовать градиенты $f'(x_k)$ неквадратичной функции вместо остатков $r_k$ и линейный поиск шага $\\alpha_k$ вместо аналитического выражения. Получим метод Флетчера-Ривса.\n",
    "\n",
    "```python\n",
    "def ConjugateGradientFR(f, gradf, x0):\n",
    "    x = x0\n",
    "    grad = gradf(x)\n",
    "    p = -grad\n",
    "    while np.linalg.norm(gradf(x)) != 0:\n",
    "        alpha = StepSearch(x, f, gradf, **kwargs)\n",
    "        x = x + alpha * p\n",
    "        grad_next = gradf(x)\n",
    "        beta = grad_next.dot(grad_next) / grad.dot(grad)\n",
    "        p = -grad_next + beta * p\n",
    "        grad = grad_next\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Теоремы сходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Комментарии\n",
    "- Замечательная методичка размещена [тут](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
    "- Помимо метода Флетчера-Ривса существуют другие способы вычисления $\\beta_k$: метод Полака-Рибьера, метод Хестенса-Штифеля...\n",
    "- Для метода сопряжённых градиентов требуется 3 вектора\n",
    "- Самой дорогой операций является умножение матрицы на вектор или вычисление скалярных произведений, помимо вычисления градиента функции\n",
    "- Методика рестартов: удаление истории $(\\beta_m = 0)$ и перезапуск алгоритма с текущей точки\n",
    "- **Отсутствует** свойство оптимальности решения на Крыловском подпространстве "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "1. Сопряжённые направления\n",
    "2. Метод сопряжённых градиентов\n",
    "3. Сходимость\n",
    "4. Эксперименты"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cvxpy]",
   "language": "python",
   "name": "conda-env-cvxpy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
